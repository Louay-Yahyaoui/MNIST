{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization, ReLU\nfrom tensorflow.keras import Model\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-03T16:10:40.865717Z","iopub.execute_input":"2023-12-03T16:10:40.866734Z","iopub.status.idle":"2023-12-03T16:10:40.877090Z","shell.execute_reply.started":"2023-12-03T16:10:40.866693Z","shell.execute_reply":"2023-12-03T16:10:40.875524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nX_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:40.879623Z","iopub.execute_input":"2023-12-03T16:10:40.880263Z","iopub.status.idle":"2023-12-03T16:10:45.455705Z","shell.execute_reply.started":"2023-12-03T16:10:40.880230Z","shell.execute_reply":"2023-12-03T16:10:45.454661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data[\"label\"]\nX = train_data.drop(\"label\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:45.456999Z","iopub.execute_input":"2023-12-03T16:10:45.457319Z","iopub.status.idle":"2023-12-03T16:10:45.537457Z","shell.execute_reply.started":"2023-12-03T16:10:45.457292Z","shell.execute_reply":"2023-12-03T16:10:45.536447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"The data is 784 pixels (28*28) and it's in grayscale so the input to the model is going to be 28 * 28 * 1 as it only has one channel.\nIt also helps for the data to be between 0 and 1 instead of 0 and 255 as it helps the model converge faster and wouldn't have massive gradients at initialization.","metadata":{}},{"cell_type":"code","source":"# Converting dataframe to tensorflow tensor then scaling the data to [0,1]\nX = tf.convert_to_tensor(X, tf.float64) / 255.0\nX_test = tf.convert_to_tensor(X_test, tf.float64) / 255.0\ny = tf.convert_to_tensor(y)\nX = tf.reshape(X, (-1, 28, 28, 1))\nX_test = tf.reshape(X_test, (-1,28,28,1))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:45.540176Z","iopub.execute_input":"2023-12-03T16:10:45.540683Z","iopub.status.idle":"2023-12-03T16:10:46.191664Z","shell.execute_reply.started":"2023-12-03T16:10:45.540654Z","shell.execute_reply":"2023-12-03T16:10:46.190847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n1 = int(X.shape[0]*.9)\nX_train, y_train = X[:n1], y[:n1]\nX_valid, y_valid = X[n1:], y[n1:]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.192814Z","iopub.execute_input":"2023-12-03T16:10:46.193133Z","iopub.status.idle":"2023-12-03T16:10:46.200495Z","shell.execute_reply.started":"2023-12-03T16:10:46.193106Z","shell.execute_reply":"2023-12-03T16:10:46.199533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"A basic CNN with 2 conv layers and 2 hidden layers, I use dropout on every single layer with p = 0.3 except for the input where I have found that it hinders performance. \n\nThere is some data augmentation as well with a random rotation below 10Â° and a random translation between 0 and .1 in both dimensions.\n","metadata":{}},{"cell_type":"code","source":"kernel_size  = 3\ndropout_rate = 0.25 #.3 in current save","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.201544Z","iopub.execute_input":"2023-12-03T16:10:46.201809Z","iopub.status.idle":"2023-12-03T16:10:46.210264Z","shell.execute_reply.started":"2023-12-03T16:10:46.201786Z","shell.execute_reply":"2023-12-03T16:10:46.209540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNIST(Model):\n    def __init__(self):\n        super().__init__()\n        self.training = True\n        \n        # Data augmentation\n        self.translate = tf.keras.layers.RandomTranslation(.1, .1)\n        self.rotate = tf.keras.layers.RandomRotation(.1)\n        \n        self.conv1 = Conv2D(6, 5, activation = \"relu\")\n        self.pool1 = MaxPooling2D()\n        self.dropout1 = Dropout(dropout_rate)\n        \n        self.conv2 = Conv2D(16, 3, activation = \"relu\")\n        #self.pool2 = MaxPooling2D()\n        self.flatten = Flatten()\n        self.dropout2 = Dropout(dropout_rate)\n        \n        self.ln1 = Dense(256, activation = \"relu\")\n        self.dropout3 = Dropout(dropout_rate)\n        \n        self.ln2 = Dense(64, activation = \"relu\")\n        self.dropout4 = Dropout(dropout_rate)\n        \n        self.out = Dense(10)\n        \n    def call(self, x):\n            \n        x = self.translate(x, training = self.training)\n        x = self.rotate(x, training = self.training)\n        \n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.dropout1(x, training = self.training)\n        \n        x = self.conv2(x)\n        #x = self.pool2(x)\n        x = self.dropout2(x, training = self.training)\n        x = self.flatten(x)\n        \n        x = self.ln1(x)\n        x = self.dropout3(x, training = self.training)\n        \n        x = self.ln2(x)\n        x = self.dropout4(x, training = self.training)\n        \n        out = self.out(x)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.211240Z","iopub.execute_input":"2023-12-03T16:10:46.211493Z","iopub.status.idle":"2023-12-03T16:10:46.222607Z","shell.execute_reply.started":"2023-12-03T16:10:46.211470Z","shell.execute_reply":"2023-12-03T16:10:46.221706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model= MNIST()\nmodel.build((None, 28, 28, 1))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.223742Z","iopub.execute_input":"2023-12-03T16:10:46.224121Z","iopub.status.idle":"2023-12-03T16:10:46.453196Z","shell.execute_reply.started":"2023-12-03T16:10:46.224096Z","shell.execute_reply":"2023-12-03T16:10:46.452305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dropout paper claims that decaying learning rate works best for NNs with dropout so that's why I chose exponential decay. As for the actual values, several were tested and this seemed to work best.","metadata":{}},{"cell_type":"code","source":"lr = tf.optimizers.schedules.ExponentialDecay(1e-3, 10000, .9)\noptimizer = tf.keras.optimizers.Adam(lr)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.455697Z","iopub.execute_input":"2023-12-03T16:10:46.455995Z","iopub.status.idle":"2023-12-03T16:10:46.462708Z","shell.execute_reply.started":"2023-12-03T16:10:46.455969Z","shell.execute_reply":"2023-12-03T16:10:46.461819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use the training and testing steps straight from the tensorflow docs with minor changes.","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(X:tf.Tensor, y:tf.Tensor):\n    model.training = True\n    \n    with tf.GradientTape() as tape:\n        predictions = model(X)\n        loss = loss_fn(y, predictions)\n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.464030Z","iopub.execute_input":"2023-12-03T16:10:46.464390Z","iopub.status.idle":"2023-12-03T16:10:46.500791Z","shell.execute_reply.started":"2023-12-03T16:10:46.464328Z","shell.execute_reply":"2023-12-03T16:10:46.500037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef test_step(images, labels):\n    model.training = False\n    predictions = model(images)\n    loss = loss_fn(labels, predictions)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.502023Z","iopub.execute_input":"2023-12-03T16:10:46.502359Z","iopub.status.idle":"2023-12-03T16:10:46.509314Z","shell.execute_reply.started":"2023-12-03T16:10:46.502327Z","shell.execute_reply":"2023-12-03T16:10:46.508359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used a fairly typical training loop.","metadata":{}},{"cell_type":"code","source":"batch_size = 512\nEPOCHS = 1000","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.510595Z","iopub.execute_input":"2023-12-03T16:10:46.510904Z","iopub.status.idle":"2023-12-03T16:10:46.516356Z","shell.execute_reply.started":"2023-12-03T16:10:46.510868Z","shell.execute_reply":"2023-12-03T16:10:46.515514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ndata = data.shuffle(buffer_size = len(X_train), reshuffle_each_iteration = True, \n                          seed = 10)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.517259Z","iopub.execute_input":"2023-12-03T16:10:46.517533Z","iopub.status.idle":"2023-12-03T16:10:46.713179Z","shell.execute_reply.started":"2023-12-03T16:10:46.517509Z","shell.execute_reply":"2023-12-03T16:10:46.712259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(1, EPOCHS+1):\n    \n    dataset = data.batch(batch_size, drop_remainder = False)\n    loss = 0\n    \n    for X_batch, y_batch in dataset:\n        loss += train_step(X_batch, y_batch) * len(X_batch)\n        \n    loss /= len(X_train)\n\n    val_loss = test_step(X_valid, y_valid)\n    val_accuracy = (np.argmax(model(X_valid), 1) == np.array(y_valid)).mean()  \n    \n    print(f\"Epoch {epoch}/{EPOCHS}: loss: {loss:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T16:10:46.714511Z","iopub.execute_input":"2023-12-03T16:10:46.714865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Putting the model in inference mode and passing the test data in batches to support GPU's memory limitations and then creating a submission that complies to the kaggle competition format.","metadata":{}},{"cell_type":"code","source":"model.training = False\ny_pred = []\n\nfor i in range(0, len(X_test)-100+1, 100):\n    partial = np.argmax(model(X_test[i:i+100]), axis = 1)\n    y_pred.append(partial)\n    \ny_pred = np.array(y_pred).reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = np.arange(start = 1, stop = y_pred.shape[0]+1)\n\nsubmission = pd.DataFrame(zip(image_id,y_pred), columns = [\"ImageId\",\"label\"])\nsubmission.set_index(\"ImageId\",inplace = True)\nsubmission.to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}