{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization, ReLU\nfrom tensorflow.keras import Model\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T10:16:03.104033Z","iopub.execute_input":"2023-12-06T10:16:03.104352Z","iopub.status.idle":"2023-12-06T10:16:11.397168Z","shell.execute_reply.started":"2023-12-06T10:16:03.104322Z","shell.execute_reply":"2023-12-06T10:16:11.396217Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nX_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:11.398920Z","iopub.execute_input":"2023-12-06T10:16:11.399501Z","iopub.status.idle":"2023-12-06T10:16:17.477815Z","shell.execute_reply.started":"2023-12-06T10:16:11.399473Z","shell.execute_reply":"2023-12-06T10:16:17.476806Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"y = train_data[\"label\"]\nX = train_data.drop(\"label\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:17.479012Z","iopub.execute_input":"2023-12-06T10:16:17.479298Z","iopub.status.idle":"2023-12-06T10:16:17.568009Z","shell.execute_reply.started":"2023-12-06T10:16:17.479272Z","shell.execute_reply":"2023-12-06T10:16:17.566902Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"The data is 784 pixels (28*28) and it's in grayscale so the input to the model is going to be 28 * 28 * 1 as it only has one channel.\nIt also helps for the data to be between 0 and 1 instead of 0 and 255 as it helps the model converge faster and wouldn't have massive gradients at initialization.","metadata":{}},{"cell_type":"code","source":"# Converting dataframe to tensorflow tensor then scaling the data to [0,1]\nX = tf.convert_to_tensor(X, tf.float64) / 255.0\nX_test = tf.convert_to_tensor(X_test, tf.float64) / 255.0\ny = tf.convert_to_tensor(y)\nX = tf.reshape(X, (-1, 28, 28, 1))\nX_test = tf.reshape(X_test, (-1,28,28,1))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:17.570329Z","iopub.execute_input":"2023-12-06T10:16:17.570623Z","iopub.status.idle":"2023-12-06T10:16:21.121804Z","shell.execute_reply.started":"2023-12-06T10:16:17.570598Z","shell.execute_reply":"2023-12-06T10:16:21.120990Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"n1 = int(X.shape[0]*.9)\nX_train, y_train = X[:n1], y[:n1]\nX_valid, y_valid = X[n1:], y[n1:]","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.122880Z","iopub.execute_input":"2023-12-06T10:16:21.123156Z","iopub.status.idle":"2023-12-06T10:16:21.139527Z","shell.execute_reply.started":"2023-12-06T10:16:21.123133Z","shell.execute_reply":"2023-12-06T10:16:21.138647Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"A basic CNN with 2 conv layers and 2 hidden layers, I use dropout on every single layer with p = 0.25 except for the input where I have found that it hinders performance. ","metadata":{}},{"cell_type":"code","source":"kernel_size  = 3\ndropout_rate = 0.25","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.140690Z","iopub.execute_input":"2023-12-06T10:16:21.141266Z","iopub.status.idle":"2023-12-06T10:16:21.145463Z","shell.execute_reply.started":"2023-12-06T10:16:21.141233Z","shell.execute_reply":"2023-12-06T10:16:21.144634Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class MNIST(Model):\n    def __init__(self):\n        super().__init__()\n        self.training = True\n        \n        # Data augmentation\n        self.translate = tf.keras.layers.RandomTranslation(.05, .05)\n        self.rotate = tf.keras.layers.RandomRotation(.05)\n        \n        self.conv1 = Conv2D(6, 5, activation = \"relu\")\n        self.pool1 = MaxPooling2D()\n        self.dropout1 = Dropout(dropout_rate)\n        \n        self.conv2 = Conv2D(16, 3, activation = \"relu\")\n        self.flatten = Flatten()\n        self.dropout2 = Dropout(dropout_rate)\n        \n        self.ln1 = Dense(256, activation = \"relu\")\n        self.dropout3 = Dropout(dropout_rate)\n        \n        self.ln2 = Dense(64, activation = \"relu\")\n        self.dropout4 = Dropout(dropout_rate)\n        \n        self.out = Dense(10)\n        \n    def call(self, x):\n            \n        x = self.translate(x, training = self.training)\n        x = self.rotate(x, training = self.training)\n        \n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.dropout1(x, training = self.training)\n        \n        x = self.conv2(x)\n        x = self.dropout2(x, training = self.training)\n        x = self.flatten(x)\n        \n        x = self.ln1(x)\n        x = self.dropout3(x, training = self.training)\n        \n        x = self.ln2(x)\n        x = self.dropout4(x, training = self.training)\n        \n        out = self.out(x)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.146526Z","iopub.execute_input":"2023-12-06T10:16:21.146778Z","iopub.status.idle":"2023-12-06T10:16:21.158510Z","shell.execute_reply.started":"2023-12-06T10:16:21.146747Z","shell.execute_reply":"2023-12-06T10:16:21.157621Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model= MNIST()\nmodel.build((None, 28, 28, 1))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.159509Z","iopub.execute_input":"2023-12-06T10:16:21.159757Z","iopub.status.idle":"2023-12-06T10:16:21.478716Z","shell.execute_reply.started":"2023-12-06T10:16:21.159727Z","shell.execute_reply":"2023-12-06T10:16:21.477822Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"mnist\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_translation (RandomT  multiple                 0         \n ranslation)                                                     \n                                                                 \n random_rotation (RandomRota  multiple                 0         \n tion)                                                           \n                                                                 \n conv2d (Conv2D)             multiple                  156       \n                                                                 \n max_pooling2d (MaxPooling2D  multiple                 0         \n )                                                               \n                                                                 \n dropout (Dropout)           multiple                  0         \n                                                                 \n conv2d_1 (Conv2D)           multiple                  880       \n                                                                 \n flatten (Flatten)           multiple                  0         \n                                                                 \n dropout_1 (Dropout)         multiple                  0         \n                                                                 \n dense (Dense)               multiple                  409856    \n                                                                 \n dropout_2 (Dropout)         multiple                  0         \n                                                                 \n dense_1 (Dense)             multiple                  16448     \n                                                                 \n dropout_3 (Dropout)         multiple                  0         \n                                                                 \n dense_2 (Dense)             multiple                  650       \n                                                                 \n=================================================================\nTotal params: 427,990\nTrainable params: 427,990\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"lr = 5e-4\noptimizer = tf.keras.optimizers.Adam(lr)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.480067Z","iopub.execute_input":"2023-12-06T10:16:21.480591Z","iopub.status.idle":"2023-12-06T10:16:21.489396Z","shell.execute_reply.started":"2023-12-06T10:16:21.480557Z","shell.execute_reply":"2023-12-06T10:16:21.488472Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"I use the training and testing steps straight from the tensorflow docs with minor changes.","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(X:tf.Tensor, y:tf.Tensor):\n    model.training = True\n    \n    with tf.GradientTape() as tape:\n        predictions = model(X)\n        loss = loss_fn(y, predictions)\n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.492252Z","iopub.execute_input":"2023-12-06T10:16:21.492595Z","iopub.status.idle":"2023-12-06T10:16:21.499135Z","shell.execute_reply.started":"2023-12-06T10:16:21.492571Z","shell.execute_reply":"2023-12-06T10:16:21.498263Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef test_step(images, labels):\n    model.training = False\n    predictions = model(images)\n    loss = loss_fn(labels, predictions)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.500194Z","iopub.execute_input":"2023-12-06T10:16:21.500461Z","iopub.status.idle":"2023-12-06T10:16:21.513423Z","shell.execute_reply.started":"2023-12-06T10:16:21.500438Z","shell.execute_reply":"2023-12-06T10:16:21.512406Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"I used a fairly typical training loop.","metadata":{}},{"cell_type":"code","source":"batch_size = 512\nEPOCHS = 500","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.514714Z","iopub.execute_input":"2023-12-06T10:16:21.515050Z","iopub.status.idle":"2023-12-06T10:16:21.526923Z","shell.execute_reply.started":"2023-12-06T10:16:21.515021Z","shell.execute_reply":"2023-12-06T10:16:21.526155Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ndata = data.shuffle(buffer_size = len(X_train), reshuffle_each_iteration = True, \n                          seed = 10)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.528059Z","iopub.execute_input":"2023-12-06T10:16:21.528396Z","iopub.status.idle":"2023-12-06T10:16:21.792709Z","shell.execute_reply.started":"2023-12-06T10:16:21.528370Z","shell.execute_reply":"2023-12-06T10:16:21.791847Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(1, EPOCHS+1):\n    \n    dataset = data.batch(batch_size, drop_remainder = False)\n    loss = 0\n    \n    for X_batch, y_batch in dataset:\n        loss += train_step(X_batch, y_batch) * len(X_batch)\n        \n    loss /= len(X_train)\n\n    val_loss = test_step(X_valid, y_valid)\n    val_accuracy = (np.argmax(model(X_valid), 1) == np.array(y_valid)).mean()  \n    \n    print(f\"Epoch {epoch}/{EPOCHS}: loss: {loss:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:16:21.794058Z","iopub.execute_input":"2023-12-06T10:16:21.794787Z","iopub.status.idle":"2023-12-06T10:20:59.910821Z","shell.execute_reply.started":"2023-12-06T10:16:21.794750Z","shell.execute_reply":"2023-12-06T10:20:59.909494Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2023-12-06 10:16:23.529633: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmnist/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n2023-12-06 10:16:32.834077: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmnist/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1000: loss: 1.43152, val_loss: 0.40924, val_accuracy: 0.88143\nEpoch 2/1000: loss: 0.76370, val_loss: 0.25922, val_accuracy: 0.91881\nEpoch 3/1000: loss: 0.57577, val_loss: 0.18154, val_accuracy: 0.94429\nEpoch 4/1000: loss: 0.46228, val_loss: 0.13945, val_accuracy: 0.95333\nEpoch 5/1000: loss: 0.39023, val_loss: 0.11127, val_accuracy: 0.96381\nEpoch 6/1000: loss: 0.33696, val_loss: 0.09944, val_accuracy: 0.96667\nEpoch 7/1000: loss: 0.29796, val_loss: 0.08423, val_accuracy: 0.97405\nEpoch 8/1000: loss: 0.27278, val_loss: 0.07333, val_accuracy: 0.97500\nEpoch 9/1000: loss: 0.24793, val_loss: 0.06762, val_accuracy: 0.97690\nEpoch 10/1000: loss: 0.22645, val_loss: 0.06141, val_accuracy: 0.98024\nEpoch 11/1000: loss: 0.21079, val_loss: 0.05997, val_accuracy: 0.98095\nEpoch 12/1000: loss: 0.20054, val_loss: 0.05767, val_accuracy: 0.98214\nEpoch 13/1000: loss: 0.19345, val_loss: 0.05060, val_accuracy: 0.98429\nEpoch 14/1000: loss: 0.18162, val_loss: 0.05275, val_accuracy: 0.98357\nEpoch 15/1000: loss: 0.17466, val_loss: 0.04792, val_accuracy: 0.98524\nEpoch 16/1000: loss: 0.16587, val_loss: 0.04511, val_accuracy: 0.98690\nEpoch 17/1000: loss: 0.15932, val_loss: 0.04361, val_accuracy: 0.98571\nEpoch 18/1000: loss: 0.15409, val_loss: 0.04067, val_accuracy: 0.98786\nEpoch 19/1000: loss: 0.14674, val_loss: 0.04185, val_accuracy: 0.98833\nEpoch 20/1000: loss: 0.14772, val_loss: 0.03894, val_accuracy: 0.98857\nEpoch 21/1000: loss: 0.14274, val_loss: 0.03609, val_accuracy: 0.98976\nEpoch 22/1000: loss: 0.13659, val_loss: 0.03597, val_accuracy: 0.98905\nEpoch 23/1000: loss: 0.13517, val_loss: 0.03333, val_accuracy: 0.98952\nEpoch 24/1000: loss: 0.13076, val_loss: 0.03480, val_accuracy: 0.98881\nEpoch 25/1000: loss: 0.12883, val_loss: 0.03244, val_accuracy: 0.98905\nEpoch 26/1000: loss: 0.12255, val_loss: 0.03171, val_accuracy: 0.98881\nEpoch 27/1000: loss: 0.11681, val_loss: 0.03037, val_accuracy: 0.99071\nEpoch 28/1000: loss: 0.11950, val_loss: 0.03209, val_accuracy: 0.99024\nEpoch 29/1000: loss: 0.11485, val_loss: 0.02953, val_accuracy: 0.98976\nEpoch 30/1000: loss: 0.11399, val_loss: 0.02905, val_accuracy: 0.98952\nEpoch 31/1000: loss: 0.11283, val_loss: 0.02977, val_accuracy: 0.98929\nEpoch 32/1000: loss: 0.10902, val_loss: 0.02988, val_accuracy: 0.99095\nEpoch 33/1000: loss: 0.10371, val_loss: 0.02934, val_accuracy: 0.99071\nEpoch 34/1000: loss: 0.10804, val_loss: 0.02762, val_accuracy: 0.99000\nEpoch 35/1000: loss: 0.10175, val_loss: 0.02826, val_accuracy: 0.99024\nEpoch 36/1000: loss: 0.10121, val_loss: 0.02705, val_accuracy: 0.99095\nEpoch 37/1000: loss: 0.09832, val_loss: 0.03047, val_accuracy: 0.98929\nEpoch 38/1000: loss: 0.09967, val_loss: 0.02713, val_accuracy: 0.99095\nEpoch 39/1000: loss: 0.09611, val_loss: 0.02509, val_accuracy: 0.99167\nEpoch 40/1000: loss: 0.09476, val_loss: 0.02614, val_accuracy: 0.99143\nEpoch 41/1000: loss: 0.09829, val_loss: 0.02467, val_accuracy: 0.99262\nEpoch 42/1000: loss: 0.09329, val_loss: 0.02481, val_accuracy: 0.99119\nEpoch 43/1000: loss: 0.08842, val_loss: 0.02302, val_accuracy: 0.99214\nEpoch 44/1000: loss: 0.09035, val_loss: 0.02190, val_accuracy: 0.99262\nEpoch 45/1000: loss: 0.09288, val_loss: 0.02381, val_accuracy: 0.99167\nEpoch 46/1000: loss: 0.08774, val_loss: 0.02489, val_accuracy: 0.99190\nEpoch 47/1000: loss: 0.08506, val_loss: 0.02266, val_accuracy: 0.99167\nEpoch 48/1000: loss: 0.08439, val_loss: 0.02363, val_accuracy: 0.99143\nEpoch 49/1000: loss: 0.08329, val_loss: 0.02296, val_accuracy: 0.99286\nEpoch 50/1000: loss: 0.08027, val_loss: 0.02367, val_accuracy: 0.99333\nEpoch 51/1000: loss: 0.08054, val_loss: 0.02417, val_accuracy: 0.99167\nEpoch 52/1000: loss: 0.07966, val_loss: 0.02348, val_accuracy: 0.99214\nEpoch 53/1000: loss: 0.08055, val_loss: 0.02440, val_accuracy: 0.99214\nEpoch 54/1000: loss: 0.08048, val_loss: 0.02419, val_accuracy: 0.99286\nEpoch 55/1000: loss: 0.07928, val_loss: 0.02297, val_accuracy: 0.99286\nEpoch 56/1000: loss: 0.07587, val_loss: 0.02074, val_accuracy: 0.99286\nEpoch 57/1000: loss: 0.07741, val_loss: 0.02163, val_accuracy: 0.99262\nEpoch 58/1000: loss: 0.07696, val_loss: 0.02164, val_accuracy: 0.99310\nEpoch 59/1000: loss: 0.07536, val_loss: 0.02117, val_accuracy: 0.99381\nEpoch 60/1000: loss: 0.07728, val_loss: 0.02413, val_accuracy: 0.99190\nEpoch 61/1000: loss: 0.07332, val_loss: 0.02104, val_accuracy: 0.99333\nEpoch 62/1000: loss: 0.07147, val_loss: 0.02171, val_accuracy: 0.99238\nEpoch 63/1000: loss: 0.07165, val_loss: 0.01993, val_accuracy: 0.99262\nEpoch 64/1000: loss: 0.06988, val_loss: 0.01974, val_accuracy: 0.99429\nEpoch 65/1000: loss: 0.07053, val_loss: 0.02152, val_accuracy: 0.99381\nEpoch 66/1000: loss: 0.07178, val_loss: 0.01957, val_accuracy: 0.99452\nEpoch 67/1000: loss: 0.06813, val_loss: 0.02145, val_accuracy: 0.99286\nEpoch 68/1000: loss: 0.07081, val_loss: 0.01916, val_accuracy: 0.99286\nEpoch 69/1000: loss: 0.06729, val_loss: 0.02178, val_accuracy: 0.99286\nEpoch 70/1000: loss: 0.06773, val_loss: 0.02039, val_accuracy: 0.99381\nEpoch 71/1000: loss: 0.06425, val_loss: 0.02032, val_accuracy: 0.99333\nEpoch 72/1000: loss: 0.06603, val_loss: 0.01946, val_accuracy: 0.99333\nEpoch 73/1000: loss: 0.06672, val_loss: 0.02070, val_accuracy: 0.99167\nEpoch 74/1000: loss: 0.06684, val_loss: 0.01990, val_accuracy: 0.99310\nEpoch 75/1000: loss: 0.06555, val_loss: 0.02034, val_accuracy: 0.99333\nEpoch 76/1000: loss: 0.06404, val_loss: 0.02108, val_accuracy: 0.99286\nEpoch 77/1000: loss: 0.06378, val_loss: 0.01957, val_accuracy: 0.99405\nEpoch 78/1000: loss: 0.06444, val_loss: 0.01912, val_accuracy: 0.99429\nEpoch 79/1000: loss: 0.06283, val_loss: 0.01988, val_accuracy: 0.99262\nEpoch 80/1000: loss: 0.06441, val_loss: 0.01781, val_accuracy: 0.99452\nEpoch 81/1000: loss: 0.06078, val_loss: 0.02215, val_accuracy: 0.99333\nEpoch 82/1000: loss: 0.06201, val_loss: 0.01963, val_accuracy: 0.99476\nEpoch 83/1000: loss: 0.06129, val_loss: 0.01962, val_accuracy: 0.99429\nEpoch 84/1000: loss: 0.05775, val_loss: 0.01948, val_accuracy: 0.99381\nEpoch 85/1000: loss: 0.06034, val_loss: 0.01995, val_accuracy: 0.99357\nEpoch 86/1000: loss: 0.05998, val_loss: 0.01639, val_accuracy: 0.99524\nEpoch 87/1000: loss: 0.05990, val_loss: 0.01883, val_accuracy: 0.99333\nEpoch 88/1000: loss: 0.05919, val_loss: 0.01796, val_accuracy: 0.99381\nEpoch 89/1000: loss: 0.06045, val_loss: 0.01746, val_accuracy: 0.99405\nEpoch 90/1000: loss: 0.05726, val_loss: 0.01557, val_accuracy: 0.99452\nEpoch 91/1000: loss: 0.05806, val_loss: 0.01757, val_accuracy: 0.99357\nEpoch 92/1000: loss: 0.05936, val_loss: 0.01752, val_accuracy: 0.99452\nEpoch 93/1000: loss: 0.06011, val_loss: 0.01520, val_accuracy: 0.99524\nEpoch 94/1000: loss: 0.05948, val_loss: 0.01485, val_accuracy: 0.99548\nEpoch 95/1000: loss: 0.05923, val_loss: 0.01495, val_accuracy: 0.99476\nEpoch 96/1000: loss: 0.05589, val_loss: 0.01552, val_accuracy: 0.99381\nEpoch 97/1000: loss: 0.05474, val_loss: 0.01908, val_accuracy: 0.99452\nEpoch 98/1000: loss: 0.05686, val_loss: 0.01661, val_accuracy: 0.99429\nEpoch 99/1000: loss: 0.05342, val_loss: 0.01584, val_accuracy: 0.99429\nEpoch 100/1000: loss: 0.05402, val_loss: 0.01623, val_accuracy: 0.99452\nEpoch 101/1000: loss: 0.05412, val_loss: 0.01446, val_accuracy: 0.99524\nEpoch 102/1000: loss: 0.05487, val_loss: 0.01500, val_accuracy: 0.99571\nEpoch 103/1000: loss: 0.05438, val_loss: 0.01574, val_accuracy: 0.99476\nEpoch 104/1000: loss: 0.05533, val_loss: 0.01682, val_accuracy: 0.99429\nEpoch 105/1000: loss: 0.05289, val_loss: 0.01699, val_accuracy: 0.99429\nEpoch 106/1000: loss: 0.05283, val_loss: 0.01509, val_accuracy: 0.99452\nEpoch 107/1000: loss: 0.05403, val_loss: 0.01462, val_accuracy: 0.99524\nEpoch 108/1000: loss: 0.05231, val_loss: 0.01448, val_accuracy: 0.99429\nEpoch 109/1000: loss: 0.05149, val_loss: 0.01575, val_accuracy: 0.99405\nEpoch 110/1000: loss: 0.05148, val_loss: 0.01525, val_accuracy: 0.99429\nEpoch 111/1000: loss: 0.05399, val_loss: 0.01391, val_accuracy: 0.99548\nEpoch 112/1000: loss: 0.05130, val_loss: 0.01470, val_accuracy: 0.99452\nEpoch 113/1000: loss: 0.04985, val_loss: 0.01522, val_accuracy: 0.99571\nEpoch 114/1000: loss: 0.05016, val_loss: 0.01793, val_accuracy: 0.99429\nEpoch 115/1000: loss: 0.05091, val_loss: 0.01835, val_accuracy: 0.99310\nEpoch 116/1000: loss: 0.04738, val_loss: 0.01627, val_accuracy: 0.99429\nEpoch 117/1000: loss: 0.04912, val_loss: 0.01750, val_accuracy: 0.99381\nEpoch 118/1000: loss: 0.04888, val_loss: 0.01648, val_accuracy: 0.99476\nEpoch 119/1000: loss: 0.04858, val_loss: 0.01633, val_accuracy: 0.99452\nEpoch 120/1000: loss: 0.04919, val_loss: 0.01650, val_accuracy: 0.99476\nEpoch 121/1000: loss: 0.04893, val_loss: 0.01651, val_accuracy: 0.99476\nEpoch 122/1000: loss: 0.05053, val_loss: 0.01502, val_accuracy: 0.99500\nEpoch 123/1000: loss: 0.04831, val_loss: 0.01591, val_accuracy: 0.99476\nEpoch 124/1000: loss: 0.04664, val_loss: 0.01681, val_accuracy: 0.99476\nEpoch 125/1000: loss: 0.04442, val_loss: 0.01593, val_accuracy: 0.99524\nEpoch 126/1000: loss: 0.04877, val_loss: 0.01580, val_accuracy: 0.99500\nEpoch 127/1000: loss: 0.04707, val_loss: 0.01453, val_accuracy: 0.99476\nEpoch 128/1000: loss: 0.04947, val_loss: 0.01378, val_accuracy: 0.99595\nEpoch 129/1000: loss: 0.04607, val_loss: 0.01448, val_accuracy: 0.99571\nEpoch 130/1000: loss: 0.04661, val_loss: 0.01501, val_accuracy: 0.99595\nEpoch 131/1000: loss: 0.04679, val_loss: 0.01248, val_accuracy: 0.99667\nEpoch 132/1000: loss: 0.04712, val_loss: 0.01376, val_accuracy: 0.99595\nEpoch 133/1000: loss: 0.04664, val_loss: 0.01485, val_accuracy: 0.99524\nEpoch 134/1000: loss: 0.04726, val_loss: 0.01360, val_accuracy: 0.99595\nEpoch 135/1000: loss: 0.04464, val_loss: 0.01234, val_accuracy: 0.99643\nEpoch 136/1000: loss: 0.04321, val_loss: 0.01559, val_accuracy: 0.99619\nEpoch 137/1000: loss: 0.04497, val_loss: 0.01499, val_accuracy: 0.99524\nEpoch 138/1000: loss: 0.04358, val_loss: 0.01484, val_accuracy: 0.99500\nEpoch 139/1000: loss: 0.04564, val_loss: 0.01481, val_accuracy: 0.99571\nEpoch 140/1000: loss: 0.04623, val_loss: 0.01344, val_accuracy: 0.99524\nEpoch 141/1000: loss: 0.04619, val_loss: 0.01351, val_accuracy: 0.99571\nEpoch 142/1000: loss: 0.04424, val_loss: 0.01391, val_accuracy: 0.99548\nEpoch 143/1000: loss: 0.04323, val_loss: 0.01564, val_accuracy: 0.99500\nEpoch 144/1000: loss: 0.04318, val_loss: 0.01405, val_accuracy: 0.99571\nEpoch 145/1000: loss: 0.04323, val_loss: 0.01266, val_accuracy: 0.99619\nEpoch 146/1000: loss: 0.04249, val_loss: 0.01412, val_accuracy: 0.99643\nEpoch 147/1000: loss: 0.04123, val_loss: 0.01561, val_accuracy: 0.99429\nEpoch 148/1000: loss: 0.04342, val_loss: 0.01542, val_accuracy: 0.99524\nEpoch 149/1000: loss: 0.04485, val_loss: 0.01437, val_accuracy: 0.99548\nEpoch 150/1000: loss: 0.04311, val_loss: 0.01492, val_accuracy: 0.99619\nEpoch 151/1000: loss: 0.04095, val_loss: 0.01295, val_accuracy: 0.99619\nEpoch 152/1000: loss: 0.04403, val_loss: 0.01574, val_accuracy: 0.99595\nEpoch 153/1000: loss: 0.04302, val_loss: 0.01333, val_accuracy: 0.99619\nEpoch 154/1000: loss: 0.04267, val_loss: 0.01309, val_accuracy: 0.99571\nEpoch 155/1000: loss: 0.04617, val_loss: 0.01392, val_accuracy: 0.99595\nEpoch 156/1000: loss: 0.04170, val_loss: 0.01356, val_accuracy: 0.99619\nEpoch 157/1000: loss: 0.04494, val_loss: 0.01377, val_accuracy: 0.99667\nEpoch 158/1000: loss: 0.04119, val_loss: 0.01587, val_accuracy: 0.99548\nEpoch 159/1000: loss: 0.04135, val_loss: 0.01378, val_accuracy: 0.99595\nEpoch 160/1000: loss: 0.04032, val_loss: 0.01390, val_accuracy: 0.99619\nEpoch 161/1000: loss: 0.04046, val_loss: 0.01275, val_accuracy: 0.99619\nEpoch 162/1000: loss: 0.04218, val_loss: 0.01383, val_accuracy: 0.99548\nEpoch 163/1000: loss: 0.03989, val_loss: 0.01411, val_accuracy: 0.99548\nEpoch 164/1000: loss: 0.04250, val_loss: 0.01402, val_accuracy: 0.99571\nEpoch 165/1000: loss: 0.04038, val_loss: 0.01650, val_accuracy: 0.99476\nEpoch 166/1000: loss: 0.04101, val_loss: 0.01421, val_accuracy: 0.99500\nEpoch 167/1000: loss: 0.04298, val_loss: 0.01494, val_accuracy: 0.99571\nEpoch 168/1000: loss: 0.04490, val_loss: 0.01546, val_accuracy: 0.99500\nEpoch 169/1000: loss: 0.04120, val_loss: 0.01456, val_accuracy: 0.99524\nEpoch 170/1000: loss: 0.04057, val_loss: 0.01493, val_accuracy: 0.99595\nEpoch 171/1000: loss: 0.03992, val_loss: 0.01496, val_accuracy: 0.99548\nEpoch 172/1000: loss: 0.04089, val_loss: 0.01621, val_accuracy: 0.99524\nEpoch 173/1000: loss: 0.03951, val_loss: 0.01461, val_accuracy: 0.99548\nEpoch 174/1000: loss: 0.03909, val_loss: 0.01445, val_accuracy: 0.99548\nEpoch 175/1000: loss: 0.03733, val_loss: 0.01522, val_accuracy: 0.99524\nEpoch 176/1000: loss: 0.03655, val_loss: 0.01743, val_accuracy: 0.99500\nEpoch 177/1000: loss: 0.04260, val_loss: 0.01629, val_accuracy: 0.99571\nEpoch 178/1000: loss: 0.04101, val_loss: 0.01606, val_accuracy: 0.99548\nEpoch 179/1000: loss: 0.03889, val_loss: 0.01485, val_accuracy: 0.99595\nEpoch 180/1000: loss: 0.04168, val_loss: 0.01557, val_accuracy: 0.99452\nEpoch 181/1000: loss: 0.03901, val_loss: 0.01564, val_accuracy: 0.99500\nEpoch 182/1000: loss: 0.03804, val_loss: 0.01485, val_accuracy: 0.99381\nEpoch 183/1000: loss: 0.03984, val_loss: 0.01591, val_accuracy: 0.99429\nEpoch 184/1000: loss: 0.03776, val_loss: 0.01650, val_accuracy: 0.99452\nEpoch 185/1000: loss: 0.03990, val_loss: 0.01661, val_accuracy: 0.99429\nEpoch 186/1000: loss: 0.03897, val_loss: 0.01615, val_accuracy: 0.99500\nEpoch 187/1000: loss: 0.04034, val_loss: 0.01426, val_accuracy: 0.99643\nEpoch 188/1000: loss: 0.03621, val_loss: 0.01539, val_accuracy: 0.99452\nEpoch 189/1000: loss: 0.03969, val_loss: 0.01553, val_accuracy: 0.99476\nEpoch 190/1000: loss: 0.03472, val_loss: 0.01481, val_accuracy: 0.99595\nEpoch 191/1000: loss: 0.03764, val_loss: 0.01361, val_accuracy: 0.99595\nEpoch 192/1000: loss: 0.03568, val_loss: 0.01535, val_accuracy: 0.99524\nEpoch 193/1000: loss: 0.03955, val_loss: 0.01389, val_accuracy: 0.99571\nEpoch 194/1000: loss: 0.03726, val_loss: 0.01471, val_accuracy: 0.99571\nEpoch 195/1000: loss: 0.03798, val_loss: 0.01405, val_accuracy: 0.99595\nEpoch 196/1000: loss: 0.03867, val_loss: 0.01301, val_accuracy: 0.99595\nEpoch 197/1000: loss: 0.03924, val_loss: 0.01427, val_accuracy: 0.99548\nEpoch 198/1000: loss: 0.03767, val_loss: 0.01421, val_accuracy: 0.99595\nEpoch 199/1000: loss: 0.03779, val_loss: 0.01457, val_accuracy: 0.99548\nEpoch 200/1000: loss: 0.03857, val_loss: 0.01354, val_accuracy: 0.99595\nEpoch 201/1000: loss: 0.03619, val_loss: 0.01426, val_accuracy: 0.99595\nEpoch 202/1000: loss: 0.03375, val_loss: 0.01534, val_accuracy: 0.99595\nEpoch 203/1000: loss: 0.03594, val_loss: 0.01546, val_accuracy: 0.99500\nEpoch 204/1000: loss: 0.03831, val_loss: 0.01427, val_accuracy: 0.99548\nEpoch 205/1000: loss: 0.03709, val_loss: 0.01539, val_accuracy: 0.99524\nEpoch 206/1000: loss: 0.03517, val_loss: 0.01443, val_accuracy: 0.99548\nEpoch 207/1000: loss: 0.03585, val_loss: 0.01507, val_accuracy: 0.99548\nEpoch 208/1000: loss: 0.03847, val_loss: 0.01388, val_accuracy: 0.99667\nEpoch 209/1000: loss: 0.04091, val_loss: 0.01561, val_accuracy: 0.99500\nEpoch 210/1000: loss: 0.03817, val_loss: 0.01580, val_accuracy: 0.99500\nEpoch 211/1000: loss: 0.03474, val_loss: 0.01399, val_accuracy: 0.99524\nEpoch 212/1000: loss: 0.03597, val_loss: 0.01582, val_accuracy: 0.99548\nEpoch 213/1000: loss: 0.03627, val_loss: 0.01566, val_accuracy: 0.99524\nEpoch 214/1000: loss: 0.03377, val_loss: 0.01410, val_accuracy: 0.99548\nEpoch 215/1000: loss: 0.03293, val_loss: 0.01481, val_accuracy: 0.99524\nEpoch 216/1000: loss: 0.03527, val_loss: 0.01367, val_accuracy: 0.99643\nEpoch 217/1000: loss: 0.03619, val_loss: 0.01562, val_accuracy: 0.99524\nEpoch 218/1000: loss: 0.03769, val_loss: 0.01409, val_accuracy: 0.99571\nEpoch 219/1000: loss: 0.03421, val_loss: 0.01423, val_accuracy: 0.99571\nEpoch 220/1000: loss: 0.03465, val_loss: 0.01340, val_accuracy: 0.99667\nEpoch 221/1000: loss: 0.03664, val_loss: 0.01314, val_accuracy: 0.99595\nEpoch 222/1000: loss: 0.03386, val_loss: 0.01424, val_accuracy: 0.99500\nEpoch 223/1000: loss: 0.03354, val_loss: 0.01405, val_accuracy: 0.99571\nEpoch 224/1000: loss: 0.03513, val_loss: 0.01469, val_accuracy: 0.99595\nEpoch 225/1000: loss: 0.03621, val_loss: 0.01257, val_accuracy: 0.99667\nEpoch 226/1000: loss: 0.03561, val_loss: 0.01331, val_accuracy: 0.99619\nEpoch 227/1000: loss: 0.03549, val_loss: 0.01329, val_accuracy: 0.99571\nEpoch 228/1000: loss: 0.03293, val_loss: 0.01472, val_accuracy: 0.99500\nEpoch 229/1000: loss: 0.03436, val_loss: 0.01285, val_accuracy: 0.99619\nEpoch 230/1000: loss: 0.03222, val_loss: 0.01302, val_accuracy: 0.99500\nEpoch 231/1000: loss: 0.03518, val_loss: 0.01337, val_accuracy: 0.99571\nEpoch 232/1000: loss: 0.03258, val_loss: 0.01532, val_accuracy: 0.99548\nEpoch 233/1000: loss: 0.03592, val_loss: 0.01343, val_accuracy: 0.99595\nEpoch 234/1000: loss: 0.03701, val_loss: 0.01296, val_accuracy: 0.99595\nEpoch 235/1000: loss: 0.03435, val_loss: 0.01412, val_accuracy: 0.99500\nEpoch 236/1000: loss: 0.03385, val_loss: 0.01573, val_accuracy: 0.99524\nEpoch 237/1000: loss: 0.03602, val_loss: 0.01517, val_accuracy: 0.99524\nEpoch 238/1000: loss: 0.03408, val_loss: 0.01391, val_accuracy: 0.99500\nEpoch 239/1000: loss: 0.03254, val_loss: 0.01339, val_accuracy: 0.99571\nEpoch 240/1000: loss: 0.03427, val_loss: 0.01520, val_accuracy: 0.99524\nEpoch 241/1000: loss: 0.03330, val_loss: 0.01417, val_accuracy: 0.99571\nEpoch 242/1000: loss: 0.03285, val_loss: 0.01393, val_accuracy: 0.99595\nEpoch 243/1000: loss: 0.03488, val_loss: 0.01513, val_accuracy: 0.99571\nEpoch 244/1000: loss: 0.03243, val_loss: 0.01595, val_accuracy: 0.99500\nEpoch 245/1000: loss: 0.03449, val_loss: 0.01500, val_accuracy: 0.99571\nEpoch 246/1000: loss: 0.03351, val_loss: 0.01435, val_accuracy: 0.99643\nEpoch 247/1000: loss: 0.03422, val_loss: 0.01369, val_accuracy: 0.99643\nEpoch 248/1000: loss: 0.03281, val_loss: 0.01458, val_accuracy: 0.99500\nEpoch 249/1000: loss: 0.03232, val_loss: 0.01345, val_accuracy: 0.99571\nEpoch 250/1000: loss: 0.03402, val_loss: 0.01430, val_accuracy: 0.99571\nEpoch 251/1000: loss: 0.03345, val_loss: 0.01323, val_accuracy: 0.99595\nEpoch 252/1000: loss: 0.03505, val_loss: 0.01439, val_accuracy: 0.99571\nEpoch 253/1000: loss: 0.03296, val_loss: 0.01287, val_accuracy: 0.99571\nEpoch 254/1000: loss: 0.03257, val_loss: 0.01405, val_accuracy: 0.99524\nEpoch 255/1000: loss: 0.03234, val_loss: 0.01390, val_accuracy: 0.99571\nEpoch 256/1000: loss: 0.03417, val_loss: 0.01386, val_accuracy: 0.99548\nEpoch 257/1000: loss: 0.03248, val_loss: 0.01535, val_accuracy: 0.99524\nEpoch 258/1000: loss: 0.03280, val_loss: 0.01569, val_accuracy: 0.99571\nEpoch 259/1000: loss: 0.03428, val_loss: 0.01476, val_accuracy: 0.99619\nEpoch 260/1000: loss: 0.03277, val_loss: 0.01486, val_accuracy: 0.99619\nEpoch 261/1000: loss: 0.03402, val_loss: 0.01388, val_accuracy: 0.99571\nEpoch 262/1000: loss: 0.03335, val_loss: 0.01332, val_accuracy: 0.99619\nEpoch 263/1000: loss: 0.03413, val_loss: 0.01335, val_accuracy: 0.99548\nEpoch 264/1000: loss: 0.03208, val_loss: 0.01371, val_accuracy: 0.99595\nEpoch 265/1000: loss: 0.03187, val_loss: 0.01068, val_accuracy: 0.99667\nEpoch 266/1000: loss: 0.03181, val_loss: 0.01292, val_accuracy: 0.99619\nEpoch 267/1000: loss: 0.03033, val_loss: 0.01224, val_accuracy: 0.99595\nEpoch 268/1000: loss: 0.03075, val_loss: 0.01389, val_accuracy: 0.99595\nEpoch 269/1000: loss: 0.03232, val_loss: 0.01385, val_accuracy: 0.99619\nEpoch 270/1000: loss: 0.03220, val_loss: 0.01244, val_accuracy: 0.99548\nEpoch 271/1000: loss: 0.03279, val_loss: 0.01284, val_accuracy: 0.99619\nEpoch 272/1000: loss: 0.03160, val_loss: 0.01342, val_accuracy: 0.99595\nEpoch 273/1000: loss: 0.03184, val_loss: 0.01275, val_accuracy: 0.99619\nEpoch 274/1000: loss: 0.03052, val_loss: 0.01514, val_accuracy: 0.99571\nEpoch 275/1000: loss: 0.03062, val_loss: 0.01440, val_accuracy: 0.99524\nEpoch 276/1000: loss: 0.03124, val_loss: 0.01624, val_accuracy: 0.99571\nEpoch 277/1000: loss: 0.03378, val_loss: 0.01484, val_accuracy: 0.99595\nEpoch 278/1000: loss: 0.03022, val_loss: 0.01387, val_accuracy: 0.99571\nEpoch 279/1000: loss: 0.03231, val_loss: 0.01304, val_accuracy: 0.99571\nEpoch 280/1000: loss: 0.03317, val_loss: 0.01368, val_accuracy: 0.99548\nEpoch 281/1000: loss: 0.03301, val_loss: 0.01238, val_accuracy: 0.99595\nEpoch 282/1000: loss: 0.03179, val_loss: 0.01355, val_accuracy: 0.99571\nEpoch 283/1000: loss: 0.03092, val_loss: 0.01599, val_accuracy: 0.99452\nEpoch 284/1000: loss: 0.02962, val_loss: 0.01638, val_accuracy: 0.99571\nEpoch 285/1000: loss: 0.03189, val_loss: 0.01473, val_accuracy: 0.99548\nEpoch 286/1000: loss: 0.03249, val_loss: 0.01461, val_accuracy: 0.99524\nEpoch 287/1000: loss: 0.03304, val_loss: 0.01195, val_accuracy: 0.99571\nEpoch 288/1000: loss: 0.03076, val_loss: 0.01187, val_accuracy: 0.99524\nEpoch 289/1000: loss: 0.03260, val_loss: 0.01357, val_accuracy: 0.99643\nEpoch 290/1000: loss: 0.03071, val_loss: 0.01404, val_accuracy: 0.99595\nEpoch 291/1000: loss: 0.03062, val_loss: 0.01429, val_accuracy: 0.99643\nEpoch 292/1000: loss: 0.03334, val_loss: 0.01351, val_accuracy: 0.99595\nEpoch 293/1000: loss: 0.03067, val_loss: 0.01343, val_accuracy: 0.99643\nEpoch 294/1000: loss: 0.03070, val_loss: 0.01503, val_accuracy: 0.99643\nEpoch 295/1000: loss: 0.03141, val_loss: 0.01630, val_accuracy: 0.99619\nEpoch 296/1000: loss: 0.03174, val_loss: 0.01493, val_accuracy: 0.99619\nEpoch 297/1000: loss: 0.02872, val_loss: 0.01430, val_accuracy: 0.99571\nEpoch 298/1000: loss: 0.03278, val_loss: 0.01529, val_accuracy: 0.99619\nEpoch 299/1000: loss: 0.03180, val_loss: 0.01506, val_accuracy: 0.99571\nEpoch 300/1000: loss: 0.03008, val_loss: 0.01506, val_accuracy: 0.99571\nEpoch 301/1000: loss: 0.02890, val_loss: 0.01547, val_accuracy: 0.99619\nEpoch 302/1000: loss: 0.03157, val_loss: 0.01637, val_accuracy: 0.99571\nEpoch 303/1000: loss: 0.03044, val_loss: 0.01613, val_accuracy: 0.99619\nEpoch 304/1000: loss: 0.03111, val_loss: 0.01578, val_accuracy: 0.99571\nEpoch 305/1000: loss: 0.02971, val_loss: 0.01628, val_accuracy: 0.99571\nEpoch 306/1000: loss: 0.03119, val_loss: 0.01541, val_accuracy: 0.99548\nEpoch 307/1000: loss: 0.03135, val_loss: 0.01481, val_accuracy: 0.99548\nEpoch 308/1000: loss: 0.03141, val_loss: 0.01482, val_accuracy: 0.99500\nEpoch 309/1000: loss: 0.02943, val_loss: 0.01485, val_accuracy: 0.99524\nEpoch 310/1000: loss: 0.02740, val_loss: 0.01359, val_accuracy: 0.99643\nEpoch 311/1000: loss: 0.03009, val_loss: 0.01331, val_accuracy: 0.99643\nEpoch 312/1000: loss: 0.02907, val_loss: 0.01633, val_accuracy: 0.99571\nEpoch 313/1000: loss: 0.02892, val_loss: 0.01706, val_accuracy: 0.99476\nEpoch 314/1000: loss: 0.03127, val_loss: 0.01571, val_accuracy: 0.99548\nEpoch 315/1000: loss: 0.03123, val_loss: 0.01581, val_accuracy: 0.99548\nEpoch 316/1000: loss: 0.02928, val_loss: 0.01539, val_accuracy: 0.99500\nEpoch 317/1000: loss: 0.02825, val_loss: 0.01667, val_accuracy: 0.99571\nEpoch 318/1000: loss: 0.03152, val_loss: 0.01777, val_accuracy: 0.99500\nEpoch 319/1000: loss: 0.02859, val_loss: 0.01718, val_accuracy: 0.99524\nEpoch 320/1000: loss: 0.03043, val_loss: 0.01599, val_accuracy: 0.99571\nEpoch 321/1000: loss: 0.02793, val_loss: 0.01643, val_accuracy: 0.99524\nEpoch 322/1000: loss: 0.03158, val_loss: 0.01649, val_accuracy: 0.99476\nEpoch 323/1000: loss: 0.02863, val_loss: 0.01761, val_accuracy: 0.99500\nEpoch 324/1000: loss: 0.03149, val_loss: 0.01554, val_accuracy: 0.99548\nEpoch 325/1000: loss: 0.02800, val_loss: 0.01517, val_accuracy: 0.99524\nEpoch 326/1000: loss: 0.02861, val_loss: 0.01521, val_accuracy: 0.99500\nEpoch 327/1000: loss: 0.02709, val_loss: 0.01455, val_accuracy: 0.99595\nEpoch 328/1000: loss: 0.02799, val_loss: 0.01589, val_accuracy: 0.99500\nEpoch 329/1000: loss: 0.03160, val_loss: 0.01623, val_accuracy: 0.99500\nEpoch 330/1000: loss: 0.02945, val_loss: 0.01574, val_accuracy: 0.99524\nEpoch 331/1000: loss: 0.02966, val_loss: 0.01550, val_accuracy: 0.99500\nEpoch 332/1000: loss: 0.02863, val_loss: 0.01579, val_accuracy: 0.99524\nEpoch 333/1000: loss: 0.02827, val_loss: 0.01342, val_accuracy: 0.99595\nEpoch 334/1000: loss: 0.02870, val_loss: 0.01588, val_accuracy: 0.99500\nEpoch 335/1000: loss: 0.02953, val_loss: 0.01514, val_accuracy: 0.99500\nEpoch 336/1000: loss: 0.02872, val_loss: 0.01498, val_accuracy: 0.99571\nEpoch 337/1000: loss: 0.03041, val_loss: 0.01638, val_accuracy: 0.99500\nEpoch 338/1000: loss: 0.02932, val_loss: 0.01540, val_accuracy: 0.99548\nEpoch 339/1000: loss: 0.02790, val_loss: 0.01546, val_accuracy: 0.99548\nEpoch 340/1000: loss: 0.03023, val_loss: 0.01441, val_accuracy: 0.99571\nEpoch 341/1000: loss: 0.02908, val_loss: 0.01604, val_accuracy: 0.99571\nEpoch 342/1000: loss: 0.02987, val_loss: 0.01544, val_accuracy: 0.99524\nEpoch 343/1000: loss: 0.02642, val_loss: 0.01565, val_accuracy: 0.99500\nEpoch 344/1000: loss: 0.02838, val_loss: 0.01684, val_accuracy: 0.99476\nEpoch 345/1000: loss: 0.02850, val_loss: 0.01650, val_accuracy: 0.99619\nEpoch 346/1000: loss: 0.02815, val_loss: 0.01546, val_accuracy: 0.99548\nEpoch 347/1000: loss: 0.03137, val_loss: 0.01487, val_accuracy: 0.99643\nEpoch 348/1000: loss: 0.02943, val_loss: 0.01715, val_accuracy: 0.99571\nEpoch 349/1000: loss: 0.02624, val_loss: 0.01576, val_accuracy: 0.99524\nEpoch 350/1000: loss: 0.02772, val_loss: 0.01541, val_accuracy: 0.99524\nEpoch 351/1000: loss: 0.02796, val_loss: 0.01582, val_accuracy: 0.99548\nEpoch 352/1000: loss: 0.02681, val_loss: 0.01448, val_accuracy: 0.99571\nEpoch 353/1000: loss: 0.02868, val_loss: 0.01470, val_accuracy: 0.99548\nEpoch 354/1000: loss: 0.03011, val_loss: 0.01456, val_accuracy: 0.99643\nEpoch 355/1000: loss: 0.02856, val_loss: 0.01512, val_accuracy: 0.99524\nEpoch 356/1000: loss: 0.02815, val_loss: 0.01565, val_accuracy: 0.99524\nEpoch 357/1000: loss: 0.02855, val_loss: 0.01513, val_accuracy: 0.99548\nEpoch 358/1000: loss: 0.02759, val_loss: 0.01436, val_accuracy: 0.99548\nEpoch 359/1000: loss: 0.02736, val_loss: 0.01633, val_accuracy: 0.99476\nEpoch 360/1000: loss: 0.02735, val_loss: 0.01567, val_accuracy: 0.99548\nEpoch 361/1000: loss: 0.02823, val_loss: 0.01453, val_accuracy: 0.99548\nEpoch 362/1000: loss: 0.02932, val_loss: 0.01416, val_accuracy: 0.99548\nEpoch 363/1000: loss: 0.02665, val_loss: 0.01443, val_accuracy: 0.99500\nEpoch 364/1000: loss: 0.02928, val_loss: 0.01514, val_accuracy: 0.99476\nEpoch 365/1000: loss: 0.03013, val_loss: 0.01605, val_accuracy: 0.99524\nEpoch 366/1000: loss: 0.02934, val_loss: 0.01641, val_accuracy: 0.99524\nEpoch 367/1000: loss: 0.02878, val_loss: 0.01564, val_accuracy: 0.99548\nEpoch 368/1000: loss: 0.02875, val_loss: 0.01317, val_accuracy: 0.99571\nEpoch 369/1000: loss: 0.02424, val_loss: 0.01415, val_accuracy: 0.99524\nEpoch 370/1000: loss: 0.02932, val_loss: 0.01486, val_accuracy: 0.99524\nEpoch 371/1000: loss: 0.03017, val_loss: 0.01465, val_accuracy: 0.99548\nEpoch 372/1000: loss: 0.02851, val_loss: 0.01361, val_accuracy: 0.99619\nEpoch 373/1000: loss: 0.02791, val_loss: 0.01502, val_accuracy: 0.99500\nEpoch 374/1000: loss: 0.02838, val_loss: 0.01501, val_accuracy: 0.99571\nEpoch 375/1000: loss: 0.02824, val_loss: 0.01388, val_accuracy: 0.99524\nEpoch 376/1000: loss: 0.02742, val_loss: 0.01411, val_accuracy: 0.99571\nEpoch 377/1000: loss: 0.02868, val_loss: 0.01494, val_accuracy: 0.99571\nEpoch 378/1000: loss: 0.02840, val_loss: 0.01336, val_accuracy: 0.99619\nEpoch 379/1000: loss: 0.02859, val_loss: 0.01299, val_accuracy: 0.99595\nEpoch 380/1000: loss: 0.02641, val_loss: 0.01327, val_accuracy: 0.99643\nEpoch 381/1000: loss: 0.02593, val_loss: 0.01556, val_accuracy: 0.99571\nEpoch 382/1000: loss: 0.02638, val_loss: 0.01520, val_accuracy: 0.99571\nEpoch 383/1000: loss: 0.02838, val_loss: 0.01633, val_accuracy: 0.99548\nEpoch 384/1000: loss: 0.02860, val_loss: 0.01523, val_accuracy: 0.99524\nEpoch 385/1000: loss: 0.03045, val_loss: 0.01575, val_accuracy: 0.99500\nEpoch 386/1000: loss: 0.02641, val_loss: 0.01457, val_accuracy: 0.99524\nEpoch 387/1000: loss: 0.02783, val_loss: 0.01393, val_accuracy: 0.99571\nEpoch 388/1000: loss: 0.02695, val_loss: 0.01466, val_accuracy: 0.99476\nEpoch 389/1000: loss: 0.02812, val_loss: 0.01421, val_accuracy: 0.99571\nEpoch 390/1000: loss: 0.02758, val_loss: 0.01408, val_accuracy: 0.99500\nEpoch 391/1000: loss: 0.02620, val_loss: 0.01347, val_accuracy: 0.99548\nEpoch 392/1000: loss: 0.02651, val_loss: 0.01362, val_accuracy: 0.99548\nEpoch 393/1000: loss: 0.02708, val_loss: 0.01236, val_accuracy: 0.99667\nEpoch 394/1000: loss: 0.02544, val_loss: 0.01291, val_accuracy: 0.99595\nEpoch 395/1000: loss: 0.02781, val_loss: 0.01347, val_accuracy: 0.99571\nEpoch 396/1000: loss: 0.02507, val_loss: 0.01264, val_accuracy: 0.99643\nEpoch 397/1000: loss: 0.02837, val_loss: 0.01309, val_accuracy: 0.99548\nEpoch 398/1000: loss: 0.02928, val_loss: 0.01460, val_accuracy: 0.99524\nEpoch 399/1000: loss: 0.02589, val_loss: 0.01281, val_accuracy: 0.99595\nEpoch 400/1000: loss: 0.02739, val_loss: 0.01361, val_accuracy: 0.99571\nEpoch 401/1000: loss: 0.02859, val_loss: 0.01448, val_accuracy: 0.99571\nEpoch 402/1000: loss: 0.02790, val_loss: 0.01319, val_accuracy: 0.99595\nEpoch 403/1000: loss: 0.02737, val_loss: 0.01353, val_accuracy: 0.99548\nEpoch 404/1000: loss: 0.02764, val_loss: 0.01207, val_accuracy: 0.99548\nEpoch 405/1000: loss: 0.02698, val_loss: 0.01243, val_accuracy: 0.99524\nEpoch 406/1000: loss: 0.02602, val_loss: 0.01303, val_accuracy: 0.99595\nEpoch 407/1000: loss: 0.02596, val_loss: 0.01266, val_accuracy: 0.99619\nEpoch 408/1000: loss: 0.02966, val_loss: 0.01200, val_accuracy: 0.99595\nEpoch 409/1000: loss: 0.02803, val_loss: 0.01320, val_accuracy: 0.99571\nEpoch 410/1000: loss: 0.02472, val_loss: 0.01162, val_accuracy: 0.99595\nEpoch 411/1000: loss: 0.02426, val_loss: 0.01254, val_accuracy: 0.99548\nEpoch 412/1000: loss: 0.02734, val_loss: 0.01304, val_accuracy: 0.99571\nEpoch 413/1000: loss: 0.02598, val_loss: 0.01336, val_accuracy: 0.99500\nEpoch 414/1000: loss: 0.02671, val_loss: 0.01368, val_accuracy: 0.99452\nEpoch 415/1000: loss: 0.02893, val_loss: 0.01327, val_accuracy: 0.99548\nEpoch 416/1000: loss: 0.02495, val_loss: 0.01540, val_accuracy: 0.99619\nEpoch 417/1000: loss: 0.02362, val_loss: 0.01442, val_accuracy: 0.99571\nEpoch 418/1000: loss: 0.02748, val_loss: 0.01549, val_accuracy: 0.99690\nEpoch 419/1000: loss: 0.02685, val_loss: 0.01341, val_accuracy: 0.99524\nEpoch 420/1000: loss: 0.02630, val_loss: 0.01308, val_accuracy: 0.99571\nEpoch 421/1000: loss: 0.02702, val_loss: 0.01390, val_accuracy: 0.99595\nEpoch 422/1000: loss: 0.02761, val_loss: 0.01441, val_accuracy: 0.99524\nEpoch 423/1000: loss: 0.02603, val_loss: 0.01369, val_accuracy: 0.99595\nEpoch 424/1000: loss: 0.02591, val_loss: 0.01336, val_accuracy: 0.99667\nEpoch 425/1000: loss: 0.02968, val_loss: 0.01379, val_accuracy: 0.99500\nEpoch 426/1000: loss: 0.02813, val_loss: 0.01255, val_accuracy: 0.99690\nEpoch 427/1000: loss: 0.02701, val_loss: 0.01494, val_accuracy: 0.99571\nEpoch 428/1000: loss: 0.02661, val_loss: 0.01509, val_accuracy: 0.99500\nEpoch 429/1000: loss: 0.02785, val_loss: 0.01681, val_accuracy: 0.99500\nEpoch 430/1000: loss: 0.02476, val_loss: 0.01580, val_accuracy: 0.99429\nEpoch 431/1000: loss: 0.02687, val_loss: 0.01456, val_accuracy: 0.99524\nEpoch 432/1000: loss: 0.02483, val_loss: 0.01583, val_accuracy: 0.99571\nEpoch 433/1000: loss: 0.02574, val_loss: 0.01403, val_accuracy: 0.99595\nEpoch 434/1000: loss: 0.02577, val_loss: 0.01440, val_accuracy: 0.99524\nEpoch 435/1000: loss: 0.02635, val_loss: 0.01413, val_accuracy: 0.99595\nEpoch 436/1000: loss: 0.02455, val_loss: 0.01550, val_accuracy: 0.99548\nEpoch 437/1000: loss: 0.02395, val_loss: 0.01393, val_accuracy: 0.99524\nEpoch 438/1000: loss: 0.02702, val_loss: 0.01298, val_accuracy: 0.99571\nEpoch 439/1000: loss: 0.02762, val_loss: 0.01484, val_accuracy: 0.99524\nEpoch 440/1000: loss: 0.02868, val_loss: 0.01670, val_accuracy: 0.99476\nEpoch 441/1000: loss: 0.02707, val_loss: 0.01673, val_accuracy: 0.99524\nEpoch 442/1000: loss: 0.02605, val_loss: 0.01438, val_accuracy: 0.99548\nEpoch 443/1000: loss: 0.02420, val_loss: 0.01634, val_accuracy: 0.99500\nEpoch 444/1000: loss: 0.02430, val_loss: 0.01578, val_accuracy: 0.99571\nEpoch 445/1000: loss: 0.02549, val_loss: 0.01400, val_accuracy: 0.99524\nEpoch 446/1000: loss: 0.02577, val_loss: 0.01532, val_accuracy: 0.99571\nEpoch 447/1000: loss: 0.02732, val_loss: 0.01420, val_accuracy: 0.99595\nEpoch 448/1000: loss: 0.02548, val_loss: 0.01445, val_accuracy: 0.99571\nEpoch 449/1000: loss: 0.02499, val_loss: 0.01494, val_accuracy: 0.99476\nEpoch 450/1000: loss: 0.02709, val_loss: 0.01254, val_accuracy: 0.99524\nEpoch 451/1000: loss: 0.02758, val_loss: 0.01338, val_accuracy: 0.99429\nEpoch 452/1000: loss: 0.02362, val_loss: 0.01319, val_accuracy: 0.99595\nEpoch 453/1000: loss: 0.02505, val_loss: 0.01433, val_accuracy: 0.99500\nEpoch 454/1000: loss: 0.02881, val_loss: 0.01391, val_accuracy: 0.99524\nEpoch 455/1000: loss: 0.02558, val_loss: 0.01598, val_accuracy: 0.99524\nEpoch 456/1000: loss: 0.02560, val_loss: 0.01417, val_accuracy: 0.99619\nEpoch 457/1000: loss: 0.02471, val_loss: 0.01551, val_accuracy: 0.99500\nEpoch 458/1000: loss: 0.02726, val_loss: 0.01446, val_accuracy: 0.99500\nEpoch 459/1000: loss: 0.02548, val_loss: 0.01401, val_accuracy: 0.99476\nEpoch 460/1000: loss: 0.02562, val_loss: 0.01646, val_accuracy: 0.99476\nEpoch 461/1000: loss: 0.02566, val_loss: 0.01378, val_accuracy: 0.99571\nEpoch 462/1000: loss: 0.02487, val_loss: 0.01491, val_accuracy: 0.99476\nEpoch 463/1000: loss: 0.02483, val_loss: 0.01461, val_accuracy: 0.99500\nEpoch 464/1000: loss: 0.02322, val_loss: 0.01595, val_accuracy: 0.99452\nEpoch 465/1000: loss: 0.02579, val_loss: 0.01661, val_accuracy: 0.99524\nEpoch 466/1000: loss: 0.02785, val_loss: 0.01532, val_accuracy: 0.99595\nEpoch 467/1000: loss: 0.02663, val_loss: 0.01403, val_accuracy: 0.99548\nEpoch 468/1000: loss: 0.02439, val_loss: 0.01463, val_accuracy: 0.99595\nEpoch 469/1000: loss: 0.02517, val_loss: 0.01714, val_accuracy: 0.99548\nEpoch 470/1000: loss: 0.02727, val_loss: 0.01512, val_accuracy: 0.99524\nEpoch 471/1000: loss: 0.02554, val_loss: 0.01578, val_accuracy: 0.99524\nEpoch 472/1000: loss: 0.02494, val_loss: 0.01595, val_accuracy: 0.99524\nEpoch 473/1000: loss: 0.02917, val_loss: 0.01678, val_accuracy: 0.99476\nEpoch 474/1000: loss: 0.02757, val_loss: 0.01678, val_accuracy: 0.99524\nEpoch 475/1000: loss: 0.02574, val_loss: 0.01478, val_accuracy: 0.99524\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_batch)\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train)\n\u001b[1;32m     12\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m test_step(X_valid, y_valid)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:142\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m--> 142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:342\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_define_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs):\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Gets a function for these inputs, defining it if necessary.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m  Caller must hold self._lock.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m      shape relaxation retracing.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m   args, kwargs, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 342\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_function_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    344\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature, \u001b[38;5;241m*\u001b[39margs[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature):])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:407\u001b[0m, in \u001b[0;36mFunctionSpec.canonicalize_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_pure:\n\u001b[1;32m    406\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m _convert_variables_to_tensors(args, kwargs)\n\u001b[0;32m--> 407\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_function_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m cast_inputs(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature)\n\u001b[1;32m    409\u001b[0m filtered_flat_args \u001b[38;5;241m=\u001b[39m filter_function_inputs(args, kwargs)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"Putting the model in inference mode and passing the test data in batches to support GPU's memory limitations and then creating a submission that complies to the kaggle competition format.","metadata":{}},{"cell_type":"code","source":"model.training = False\ny_pred = []\n\nfor i in range(0, len(X_test)-100+1, 100):\n    partial = np.argmax(model(X_test[i:i+100]), axis = 1)\n    y_pred.append(partial)\n    \ny_pred = np.array(y_pred).reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:20:59.911908Z","iopub.status.idle":"2023-12-06T10:20:59.912285Z","shell.execute_reply.started":"2023-12-06T10:20:59.912112Z","shell.execute_reply":"2023-12-06T10:20:59.912130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = np.arange(start = 1, stop = y_pred.shape[0]+1)\n\nsubmission = pd.DataFrame(zip(image_id,y_pred), columns = [\"ImageId\",\"label\"])\nsubmission.set_index(\"ImageId\",inplace = True)\nsubmission.to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T10:20:59.914116Z","iopub.status.idle":"2023-12-06T10:20:59.914472Z","shell.execute_reply.started":"2023-12-06T10:20:59.914306Z","shell.execute_reply":"2023-12-06T10:20:59.914322Z"},"trusted":true},"execution_count":null,"outputs":[]}]}