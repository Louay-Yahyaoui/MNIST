{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, ReLU, Layer\nfrom tensorflow.keras import Model\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T13:19:12.004055Z","iopub.execute_input":"2023-10-01T13:19:12.004927Z","iopub.status.idle":"2023-10-01T13:19:20.427786Z","shell.execute_reply.started":"2023-10-01T13:19:12.004892Z","shell.execute_reply":"2023-10-01T13:19:20.426864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nX_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:20.432197Z","iopub.execute_input":"2023-10-01T13:19:20.434543Z","iopub.status.idle":"2023-10-01T13:19:25.052903Z","shell.execute_reply.started":"2023-10-01T13:19:20.434507Z","shell.execute_reply":"2023-10-01T13:19:25.052002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data[\"label\"]\nX = train_data.drop(\"label\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:25.054302Z","iopub.execute_input":"2023-10-01T13:19:25.054837Z","iopub.status.idle":"2023-10-01T13:19:25.140576Z","shell.execute_reply.started":"2023-10-01T13:19:25.054804Z","shell.execute_reply":"2023-10-01T13:19:25.139243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# Converting dataframe to tensorflow tensor then scaling the data to [0,1]\nX = tf.convert_to_tensor(X, tf.float64) / 255.0\nX_test = tf.convert_to_tensor(X_test, tf.float64) / 255.0\ny = tf.convert_to_tensor(y)\nX = tf.reshape(X, (-1, 28, 28, 1))\nX_test = tf.reshape(X_test, (-1,28,28,1))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:25.143934Z","iopub.execute_input":"2023-10-01T13:19:25.144556Z","iopub.status.idle":"2023-10-01T13:19:28.570921Z","shell.execute_reply.started":"2023-10-01T13:19:25.144511Z","shell.execute_reply":"2023-10-01T13:19:28.569990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n1 = int(X.shape[0]*.9)\nX_train, y_train = X[:n1], y[:n1]\nX_valid, y_valid = X[n1:], y[n1:]","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.572240Z","iopub.execute_input":"2023-10-01T13:19:28.572770Z","iopub.status.idle":"2023-10-01T13:19:28.590942Z","shell.execute_reply.started":"2023-10-01T13:19:28.572739Z","shell.execute_reply":"2023-10-01T13:19:28.590099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"A basic CNN with 2 conv layers and 2 hidden layers, I use dropout on every single layer with p = 0.3 except for the input where I have found that it hinders performance. \n\nThere is some data augmentation as well with a random rotation below 10Â° and a random translation between 0 and .1 in both dimensions.\n","metadata":{}},{"cell_type":"code","source":"kernel_size  = 3\ndropout_rate = .3","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.592087Z","iopub.execute_input":"2023-10-01T13:19:28.592866Z","iopub.status.idle":"2023-10-01T13:19:28.596967Z","shell.execute_reply.started":"2023-10-01T13:19:28.592817Z","shell.execute_reply":"2023-10-01T13:19:28.596042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNIST(Model):\n    def __init__(self):\n        super().__init__()\n        self.training = True\n        \n        self.translate = tf.keras.layers.RandomTranslation(.1, .1)\n        self.rotate = tf.keras.layers.RandomRotation(.03)\n        \n        self.conv1 = Conv2D(64, 3, activation = \"relu\")\n        self.dropout1 = Dropout(dropout_rate)\n        \n        self.conv2 = Conv2D(30, 3, activation = \"relu\")\n        \n        self.flatten = Flatten()\n        \n        self.dropout2 = Dropout(dropout_rate)\n        self.ln1 = Dense(1024, activation = \"relu\")\n        self.dropout3 = Dropout(dropout_rate)\n        \n        self.ln1 = Dense(1024, activation = \"relu\")\n        self.dropout3 = Dropout(dropout_rate)\n        \n        self.ln2 = Dense(2048, activation = \"relu\")\n        self.dropout4 = Dropout(dropout_rate)\n        \n        self.out = Dense(10)\n        \n    def call(self, x):\n            \n        x = self.translate(x, training = self.training)\n        x = self.rotate(x, training = self.training)\n        \n        x = self.conv1(x)\n        \n        x = self.dropout1(x, training = self.training)\n        \n        x = self.conv2(x)\n        \n        x = self.dropout2(x, training = self.training)\n        x = self.flatten(x)\n        \n        x = self.ln1(x)\n        x = self.dropout3(x, training = self.training)\n        \n        x = self.ln2(x)\n        x = self.dropout4(x, training = self.training)\n        \n        out = self.out(x)\n        \n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.598465Z","iopub.execute_input":"2023-10-01T13:19:28.599043Z","iopub.status.idle":"2023-10-01T13:19:28.608880Z","shell.execute_reply.started":"2023-10-01T13:19:28.599013Z","shell.execute_reply":"2023-10-01T13:19:28.607895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model= MNIST()\nmodel.build((None, 28, 28, 1))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.610219Z","iopub.execute_input":"2023-10-01T13:19:28.610759Z","iopub.status.idle":"2023-10-01T13:19:28.887593Z","shell.execute_reply.started":"2023-10-01T13:19:28.610730Z","shell.execute_reply":"2023-10-01T13:19:28.886824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dropout paper claims that decaying learning rate works best for NNs with dropout so that's why I chose exponential decay. As for the actual values, several were tested and this seemed to work best.","metadata":{}},{"cell_type":"code","source":"lr = tf.keras.optimizers.schedules.ExponentialDecay(5e-4, 500, .9)\noptimizer = tf.keras.optimizers.Adam(lr)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.888607Z","iopub.execute_input":"2023-10-01T13:19:28.888941Z","iopub.status.idle":"2023-10-01T13:19:28.915892Z","shell.execute_reply.started":"2023-10-01T13:19:28.888910Z","shell.execute_reply":"2023-10-01T13:19:28.914994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use the training and testing steps straight from the tensorflow docs with minor changes.","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(X:tf.Tensor, y:tf.Tensor):\n    model.training = True\n    \n    with tf.GradientTape() as tape:\n        predictions = model(X)\n        loss = loss_fn(y, predictions)\n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.919193Z","iopub.execute_input":"2023-10-01T13:19:28.919722Z","iopub.status.idle":"2023-10-01T13:19:28.925887Z","shell.execute_reply.started":"2023-10-01T13:19:28.919691Z","shell.execute_reply":"2023-10-01T13:19:28.924545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef test_step(images, labels):\n    model.training = False\n    predictions = model(images)\n    loss = loss_fn(labels, predictions)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.927276Z","iopub.execute_input":"2023-10-01T13:19:28.927922Z","iopub.status.idle":"2023-10-01T13:19:28.939089Z","shell.execute_reply.started":"2023-10-01T13:19:28.927891Z","shell.execute_reply":"2023-10-01T13:19:28.938203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used a fairly typical training loop.","metadata":{}},{"cell_type":"code","source":"batch_size = 128\nEPOCHS = 60","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.940196Z","iopub.execute_input":"2023-10-01T13:19:28.940753Z","iopub.status.idle":"2023-10-01T13:19:28.949055Z","shell.execute_reply.started":"2023-10-01T13:19:28.940724Z","shell.execute_reply":"2023-10-01T13:19:28.948244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfor epoch in range(1, EPOCHS+1):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    dataset = dataset.shuffle(buffer_size = len(X_train), seed = 10)\n    dataset = dataset.batch(batch_size, drop_remainder = False)\n    iterator = iter(dataset)\n    loss = 0\n    \n    for i in range(0, X_train.shape[0] - batch_size, batch_size): \n        X_batch, y_batch = next(iterator)\n        loss += train_step(X_batch, y_batch)\n\n    loss *= (batch_size/len(X))\n\n    val_loss = test_step(X_valid, y_valid)\n    val_accuracy = (np.argmax(model(X_valid), 1) == np.array(y_valid)).mean()  \n    \n    print(f\"Epoch {epoch}/{EPOCHS}: loss: {loss}, val_loss: {val_loss}, val_accuracy: {val_accuracy}\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.950224Z","iopub.execute_input":"2023-10-01T13:19:28.950708Z","iopub.status.idle":"2023-10-01T13:19:28.961223Z","shell.execute_reply.started":"2023-10-01T13:19:28.950679Z","shell.execute_reply":"2023-10-01T13:19:28.960320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the final training loop (without validation).","metadata":{}},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    dataset = dataset.shuffle(buffer_size = len(X), seed = 10)\n    dataset = dataset.batch(batch_size, drop_remainder = False)\n    iterator = iter(dataset)\n    loss = 0\n    for i in range(0, X.shape[0] - batch_size, batch_size): \n        X_batch, y_batch = next(iterator)\n        loss += train_step(X_batch, y_batch)\n        \n    loss *= (batch_size/len(X))\n    print(f\"Epoch {epoch+1}/{EPOCHS}: loss: {loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:19:28.962624Z","iopub.execute_input":"2023-10-01T13:19:28.963234Z","iopub.status.idle":"2023-10-01T13:23:49.433481Z","shell.execute_reply.started":"2023-10-01T13:19:28.963206Z","shell.execute_reply":"2023-10-01T13:23:49.432505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Putting the model in inference mode and passing the test data in batches to support GPU's memory limitations and then creating a submission that complies to the kaggle competition format.","metadata":{}},{"cell_type":"code","source":"model.training = False\ny_pred = []\n\nfor i in range(0, len(X_test)-100+1, 100):\n    partial = np.argmax(model(X_test[i:i+100]), axis=1)\n    y_pred.append(partial)\n    \ny_pred= np.array(y_pred).reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:23:49.434997Z","iopub.execute_input":"2023-10-01T13:23:49.435336Z","iopub.status.idle":"2023-10-01T13:23:51.052518Z","shell.execute_reply.started":"2023-10-01T13:23:49.435305Z","shell.execute_reply":"2023-10-01T13:23:51.051600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = np.arange(start=1,stop=y_pred.shape[0]+1)\n\nsubmission=pd.DataFrame(zip(image_id,y_pred),columns=[\"ImageId\",\"label\"])\nsubmission.set_index(\"ImageId\",inplace=True)\nsubmission.to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:23:51.053652Z","iopub.execute_input":"2023-10-01T13:23:51.053985Z","iopub.status.idle":"2023-10-01T13:23:51.229716Z","shell.execute_reply.started":"2023-10-01T13:23:51.053954Z","shell.execute_reply":"2023-10-01T13:23:51.228815Z"},"trusted":true},"execution_count":null,"outputs":[]}]}