{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout\nfrom tensorflow.keras import Model\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T13:41:56.164660Z","iopub.execute_input":"2023-10-01T13:41:56.165269Z","iopub.status.idle":"2023-10-01T13:42:04.915851Z","shell.execute_reply.started":"2023-10-01T13:41:56.165235Z","shell.execute_reply":"2023-10-01T13:42:04.914838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nX_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:04.917701Z","iopub.execute_input":"2023-10-01T13:42:04.918384Z","iopub.status.idle":"2023-10-01T13:42:09.147408Z","shell.execute_reply.started":"2023-10-01T13:42:04.918351Z","shell.execute_reply":"2023-10-01T13:42:09.146466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data[\"label\"]\nX = train_data.drop(\"label\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:09.148916Z","iopub.execute_input":"2023-10-01T13:42:09.149511Z","iopub.status.idle":"2023-10-01T13:42:09.235332Z","shell.execute_reply.started":"2023-10-01T13:42:09.149454Z","shell.execute_reply":"2023-10-01T13:42:09.234259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"The data is 784 pixels (28*28) and it's in grayscale so the input to the model is going to be 28 * 28 * 1 as it only has one channel.\nIt also helps for the data to be between 0 and 1 instead of 0 and 255 as it helps the model converge faster and wouldn't have massive gradients at initialization.","metadata":{}},{"cell_type":"code","source":"# Converting dataframe to tensorflow tensor then scaling the data to [0,1]\nX = tf.convert_to_tensor(X, tf.float64) / 255.0\nX_test = tf.convert_to_tensor(X_test, tf.float64) / 255.0\ny = tf.convert_to_tensor(y)\nX = tf.reshape(X, (-1, 28, 28, 1))\nX_test = tf.reshape(X_test, (-1,28,28,1))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:09.238328Z","iopub.execute_input":"2023-10-01T13:42:09.238921Z","iopub.status.idle":"2023-10-01T13:42:13.379737Z","shell.execute_reply.started":"2023-10-01T13:42:09.238888Z","shell.execute_reply":"2023-10-01T13:42:13.378749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n1 = int(X.shape[0]*.9)\nX_train, y_train = X[:n1], y[:n1]\nX_valid, y_valid = X[n1:], y[n1:]","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.384065Z","iopub.execute_input":"2023-10-01T13:42:13.386443Z","iopub.status.idle":"2023-10-01T13:42:13.408611Z","shell.execute_reply.started":"2023-10-01T13:42:13.386410Z","shell.execute_reply":"2023-10-01T13:42:13.407795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"A basic CNN with 2 conv layers and 2 hidden layers, I use dropout on every single layer with p = 0.3 except for the input where I have found that it hinders performance. \n\nThere is some data augmentation as well with a random rotation below 10Â° and a random translation between 0 and .1 in both dimensions.\n","metadata":{}},{"cell_type":"code","source":"kernel_size  = 3\ndropout_rate = .3","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.412154Z","iopub.execute_input":"2023-10-01T13:42:13.414407Z","iopub.status.idle":"2023-10-01T13:42:13.419953Z","shell.execute_reply.started":"2023-10-01T13:42:13.414375Z","shell.execute_reply":"2023-10-01T13:42:13.419036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNIST(Model):\n    def __init__(self):\n        super().__init__()\n        self.training = True\n        \n        self.translate = tf.keras.layers.RandomTranslation(.1, .1)\n        self.rotate = tf.keras.layers.RandomRotation(.03)\n        \n        self.conv1 = Conv2D(64, kernel_size, activation = \"relu\")\n        self.dropout1 = Dropout(dropout_rate)\n        \n        self.conv2 = Conv2D(30, kernel_size, activation = \"relu\")\n        self.flatten = Flatten()\n        self.dropout2 = Dropout(dropout_rate)\n        \n        self.ln1 = Dense(1024, activation = \"relu\")\n        self.dropout3 = Dropout(dropout_rate)\n        \n        self.ln2 = Dense(2048, activation = \"relu\")\n        self.dropout4 = Dropout(dropout_rate)\n        \n        self.out = Dense(10)\n        \n    def call(self, x):\n            \n        x = self.translate(x, training = self.training)\n        x = self.rotate(x, training = self.training)\n        \n        x = self.conv1(x)\n        x = self.dropout1(x, training = self.training)\n        \n        x = self.conv2(x)\n        x = self.dropout2(x, training = self.training)\n        x = self.flatten(x)\n        \n        x = self.ln1(x)\n        x = self.dropout3(x, training = self.training)\n        \n        x = self.ln2(x)\n        x = self.dropout4(x, training = self.training)\n        \n        out = self.out(x)\n        \n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.421291Z","iopub.execute_input":"2023-10-01T13:42:13.421932Z","iopub.status.idle":"2023-10-01T13:42:13.433726Z","shell.execute_reply.started":"2023-10-01T13:42:13.421901Z","shell.execute_reply":"2023-10-01T13:42:13.432830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model= MNIST()\nmodel.build((None, 28, 28, 1))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.435051Z","iopub.execute_input":"2023-10-01T13:42:13.435611Z","iopub.status.idle":"2023-10-01T13:42:13.815882Z","shell.execute_reply.started":"2023-10-01T13:42:13.435582Z","shell.execute_reply":"2023-10-01T13:42:13.815140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dropout paper claims that decaying learning rate works best for NNs with dropout so that's why I chose exponential decay. As for the actual values, several were tested and this seemed to work best.","metadata":{}},{"cell_type":"code","source":"lr = tf.keras.optimizers.schedules.ExponentialDecay(5e-4, 500, .9)\noptimizer = tf.keras.optimizers.Adam(lr)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.816884Z","iopub.execute_input":"2023-10-01T13:42:13.817212Z","iopub.status.idle":"2023-10-01T13:42:13.854544Z","shell.execute_reply.started":"2023-10-01T13:42:13.817181Z","shell.execute_reply":"2023-10-01T13:42:13.853532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use the training and testing steps straight from the tensorflow docs with minor changes.","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(X:tf.Tensor, y:tf.Tensor):\n    model.training = True\n    \n    with tf.GradientTape() as tape:\n        predictions = model(X)\n        loss = loss_fn(y, predictions)\n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.857642Z","iopub.execute_input":"2023-10-01T13:42:13.858187Z","iopub.status.idle":"2023-10-01T13:42:13.872185Z","shell.execute_reply.started":"2023-10-01T13:42:13.858153Z","shell.execute_reply":"2023-10-01T13:42:13.871317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef test_step(images, labels):\n    model.training = False\n    predictions = model(images)\n    loss = loss_fn(labels, predictions)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.873456Z","iopub.execute_input":"2023-10-01T13:42:13.875183Z","iopub.status.idle":"2023-10-01T13:42:13.886361Z","shell.execute_reply.started":"2023-10-01T13:42:13.875150Z","shell.execute_reply":"2023-10-01T13:42:13.885405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used a fairly typical training loop.","metadata":{}},{"cell_type":"code","source":"batch_size = 128\nEPOCHS = 60","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.887648Z","iopub.execute_input":"2023-10-01T13:42:13.888202Z","iopub.status.idle":"2023-10-01T13:42:13.904557Z","shell.execute_reply.started":"2023-10-01T13:42:13.888164Z","shell.execute_reply":"2023-10-01T13:42:13.903410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfor epoch in range(1, EPOCHS+1):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    dataset = dataset.shuffle(buffer_size = len(X_train), seed = 10)\n    dataset = dataset.batch(batch_size, drop_remainder = False)\n    iterator = iter(dataset)\n    loss = 0\n    \n    for i in range(0, X_train.shape[0] - batch_size, batch_size): \n        X_batch, y_batch = next(iterator)\n        loss += train_step(X_batch, y_batch)\n\n    loss *= (batch_size/len(X))\n\n    val_loss = test_step(X_valid, y_valid)\n    val_accuracy = (np.argmax(model(X_valid), 1) == np.array(y_valid)).mean()  \n    \n    print(f\"Epoch {epoch}/{EPOCHS}: loss: {loss}, val_loss: {val_loss}, val_accuracy: {val_accuracy}\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.905607Z","iopub.execute_input":"2023-10-01T13:42:13.905901Z","iopub.status.idle":"2023-10-01T13:42:13.919433Z","shell.execute_reply.started":"2023-10-01T13:42:13.905872Z","shell.execute_reply":"2023-10-01T13:42:13.918302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the final training loop (without validation).","metadata":{}},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    dataset = dataset.shuffle(buffer_size = len(X), seed = 10)\n    dataset = dataset.batch(batch_size, drop_remainder = False)\n    iterator = iter(dataset)\n    loss = 0\n    \n    for i in range(0, X.shape[0] - batch_size, batch_size): \n        X_batch, y_batch = next(iterator)\n        loss += train_step(X_batch, y_batch)\n        \n    loss *= (batch_size/len(X))\n    print(f\"Epoch {epoch+1}/{EPOCHS}: loss: {loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:13.920952Z","iopub.execute_input":"2023-10-01T13:42:13.921629Z","iopub.status.idle":"2023-10-01T13:42:37.387829Z","shell.execute_reply.started":"2023-10-01T13:42:13.921597Z","shell.execute_reply":"2023-10-01T13:42:37.385067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Putting the model in inference mode and passing the test data in batches to support GPU's memory limitations and then creating a submission that complies to the kaggle competition format.","metadata":{}},{"cell_type":"code","source":"model.training = False\ny_pred = []\n\nfor i in range(0, len(X_test)-100+1, 100):\n    partial = np.argmax(model(X_test[i:i+100]), axis = 1)\n    y_pred.append(partial)\n    \ny_pred = np.array(y_pred).reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:41.141358Z","iopub.execute_input":"2023-10-01T13:42:41.142043Z","iopub.status.idle":"2023-10-01T13:42:42.749080Z","shell.execute_reply.started":"2023-10-01T13:42:41.142005Z","shell.execute_reply":"2023-10-01T13:42:42.748001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = np.arange(start = 1, stop = y_pred.shape[0]+1)\n\nsubmission = pd.DataFrame(zip(image_id,y_pred), columns = [\"ImageId\",\"label\"])\nsubmission.set_index(\"ImageId\",inplace = True)\nsubmission.to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T13:42:42.751071Z","iopub.execute_input":"2023-10-01T13:42:42.751787Z","iopub.status.idle":"2023-10-01T13:42:42.920138Z","shell.execute_reply.started":"2023-10-01T13:42:42.751752Z","shell.execute_reply":"2023-10-01T13:42:42.919160Z"},"trusted":true},"execution_count":null,"outputs":[]}]}